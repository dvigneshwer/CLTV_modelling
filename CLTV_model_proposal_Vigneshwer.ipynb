{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:block\">\n",
    "    <div style=\"width: 59%; display: inline-block\">\n",
    "        <h1  style=\"text-align: left\">Customer Lifetime Value Modeling</h1>\n",
    "        <div style=\"width: 100%; text-align: left; display: inline-block;\"><i>Author:</i> <strong>Vigneshwer D</strong> </div>\n",
    "    </div>\n",
    "    <div style=\"width: 20%; text-align: right; display: inline-block;\">\n",
    "        <div style=\"width: 100%; text-align: right; display: inline-block;\">\n",
    "            <i>Created: </i>\n",
    "            <time datetime=\"2016-01-06\" pubdate>April 22, 2018</time>\n",
    "        </div>\n",
    "        <div style=\"width: 100%; text-align: right; display: inline-block;\">\n",
    "            <i>Modified: </i>\n",
    "            <time datetime=\"2016-01-06\" pubdate>April 22, 2018</time>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective:\n",
    "\n",
    "## 1.1 Problem statements:\n",
    "\n",
    "* Customer Success team of companies want to assess the different factors that leads to retention among their customers and define a new customer engagement metric using statistical modelling and data analysis\n",
    "* Building customer lifetime value models for the Growth Marketing team and use them to direct efforts for maximum impact and improve conversion rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Research:\n",
    "\n",
    "**Customer lifetime value (CLV)** is defined as the discounted value of future profits generated by a customer. The profits here includes the cost and revenue, out of these two metrics the revenue generated from a customer is of great interest to the business. \n",
    "\n",
    "If we are able to statistically determine the revenue that we can get from a customer, it will help the business to make critical decision about investments that can be made on a customer. \n",
    "\n",
    "Using statistical techniqes we want to analyze customer patterns from historical transactional data and use these analysis to design accurate predictive models keeping in mind the business context.\n",
    "\n",
    "According to the purchase section of the siteminder site, we can broadly classify the business context of siteminder site to fall under the category of discrete purchase and non-contractual setting.\n",
    "\n",
    "![B_models](./images/models.png)\n",
    "\n",
    "Machine learning and Markov models are also worthy approaches to CLV modeling and need less to say there are a large number of probabilistic models have been built to address the challenges of modeling lifetime value based on various business context\n",
    "\n",
    "Many companies use **customer retention modelling** along with a loyalty programme to retain or re-activate customers. If you can identify those customers most at risk of lapsing and then offer them a suitable incentive to stay with your company, you can significantly increase profits. \n",
    "\n",
    "Here the model is to trying to predict propensity to lapse or propensity to renew. The two main propensity-modelling methods used in CRM and database marketing are regression and decision tree techniques.\n",
    "\n",
    "Logistic regression uses several variables (transactional, lifestyle or demographic data usually) to predict an outcome - eg whether a customer will lapse or not. Techniques such as CHAID or CART, allow analysts to produce a tree diagram showing predictor variables for behaviour such as propensity to lapse. \n",
    "\n",
    "## 2.1 Data \n",
    "\n",
    "![data](./images/data_warehouse.png)\n",
    "\n",
    "There are two types of data that are required for customer analytics: \n",
    "* the attributes of the entities,\n",
    "* the events in which the entities participate\n",
    "\n",
    "Entities can be customers, products, channels, or agents. Attributes of the customer include demographics, income, age, gender, location, etc.\n",
    "\n",
    "Attributes of the product include type, price, quality, etc. Attributes of the channel include type, frequency of use, and response times. Attributes of the agents include handle times, resolution rates, etc. These attributes includings what customer will buy and how much will pay.\n",
    "\n",
    "The second type of data is interactions data. These are events related to what the customer does with the company. Events come with a timestamp. Events include interactions like browsing, emails, chats, phone calls. They also include transactions like purchasing, filing a complaint, delivery of a product, payment, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis:\n",
    "\n",
    "There are certain industrial proven techniques and framework followed by most of the data science companies to perform data wrangling inorder to account fo null values etc and EDA on the data to understand the probablity distrbution of different variables, correlation & many more\n",
    "\n",
    "Here are few techniques to prepare your data: \n",
    "\n",
    "1. Start with variable type analysis, where we analyze the frequency and column types\n",
    "\n",
    "2. Next perform **descriptive statistics** to find statistical information pertaining to the variables present in the data set\n",
    "    * For numerical data, we find out the unique, missing, min, max , mean, median, SD, skewness, kurtosis etc \n",
    "    * For categorical columns, unique missing and mode is usually calculated\n",
    "    \n",
    "3. We then plot the **univariate distribution plots** to visually inspect the probability density for continous variable and bar plots for categorical variable\n",
    "\n",
    "4. **Treating missing values** is an important step, where we usually remove null values with mean, median, binning and regression techniques incase of continous variables and mode incase of categorical variables\n",
    "\n",
    "5. Next up in **outlier treatment and analysis**: \n",
    "    * Many algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the results and make results less reliable, that’s why we want to recognize all the outliers and remove them.\n",
    "    * By performing univariate outlier analysis, we will find outliers with respect to one variable based on IQR, percentile and z score\n",
    "    * Values ranging outside these ranges can be removed tagging them as outliers\n",
    "\n",
    "6. **Multivariate outlier detection** is the important task of statistical analysis of multivariate data. There are various methods that can be applied to a set of data to illustrate the multiple outlier detection procedure. In this process we build a linear model to fit the data, and depending on how far the data point is from the regression line and its corresponding p value we decide if its an outlier or not. We choose independent variable and dependent variables to build the linear model and the outliers detection process will depend on the model generated that is the reason these variables should be chosen carefully.\n",
    "\n",
    "7. Creation of correlation matrix, highly correlated variables will have correlation value close to +1 and less correlated variables will have correlation value close to -1\n",
    "\n",
    "![Correlation Matrix](./images/correlation_matrix.png)\n",
    "\n",
    "8. Other techniques that are usually performed are:\n",
    "    * **Bivariate Distribution Plots: **Analyze the relationship between two variables through scatter plot\n",
    "    * **Trellis Plot: ** These plots are based on the idea of conditioning on the values taken on by one or more of the variables in a data set. \n",
    "        * In the case of a categorical variable, this means carrying out the same plot for the data subsets corresponding to each of the levels of that variable.\n",
    "        * In the case of a numeric variable, it means carrying out the same plot for the data subsets corresponding to intervals(in our case we have taken these intervels to be quartiles) of that variable\n",
    "    * ** Correlation Network: ** In the correlation network all the variables are represented as nodes and the connections among the nodes are known as edges. We can tell about the relation between any two variables by looking at the color and strength of the color of the edge connecting these two variables. Dark green represents +1 correlation and dark red represents -1 correlation\n",
    "    ![correlation matrix](./images/correlation_network.png)\n",
    "    * **Factor Analysis: ** The scree plot of the eigen values plotted against the factor numbers, this plot is used to decide the number of factors the user would like to use for the analysis. \n",
    "    Higher the eigen value of the factor higher variance does it explain, we always want to select the factors with maximum eigen value. \n",
    "    There are two common approaches to choose number of factors : \n",
    "        * Select the number of factors with eigenvalues of 1.00 or higher \n",
    "        * The number of factors appropriate for a particular analysis is the number of factors before the plotted line turns sharply right\n",
    "        \n",
    "![](./images/scree_plot.png)\n",
    "![](./images/factor_analysis.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithmic Solutions:\n",
    "\n",
    "### 4.1 Model Architecture\n",
    "\n",
    "![Architecture](./images/arch.png)\n",
    "\n",
    "The class of algorithms we could use would be regression to come up with a retension score or we can use classification algorithms to come up with the retension probability\n",
    "\n",
    "### 4.2 Clutering \n",
    "\n",
    "![Clusters](./images/clutering.png)\n",
    "\n",
    "### 4.3 ML models \n",
    "\n",
    "### Random Forest\n",
    "\n",
    "The random-forest algorithm brings extra randomness into the model, when it is growing the trees. Instead of searching for the best feature while splitting a node, it searches for the best feature among a random subset of features. This process creates a wide diversity, which generally results in a better model.\n",
    "\n",
    "Therefore when you are growing a tree in random forest, only a random subset of the features is considered for splitting a node. You can even make trees more random, by using random thresholds on top of it, for each feature rather than searching for the best possible thresholds (like a normal decision tree does).\n",
    "\n",
    "![Random Forest](./images/rf.png)\n",
    "\n",
    "### Naive bayes\n",
    "\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:\n",
    "\n",
    "![Bayes theorm](./images/bayes.png)\n",
    "\n",
    "* P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).\n",
    "* P(c) is the prior probability of class.\n",
    "* P(x|c) is the likelihood which is the probability of predictor given class.\n",
    "* P(x) is the prior probability of predictor.\n",
    "\n",
    "### Regression Analysis\n",
    "\n",
    "Regression analysis is a form of predictive modelling technique which investigates the relationship between a dependent (target) and independent variable (s) (predictor). This technique is used for forecasting, time series modelling and finding the causal effect relationship between the variables\n",
    "\n",
    "There are multiple benefits of using regression analysis. They are as follows:\n",
    "\n",
    "* It indicates the significant relationships between dependent variable and independent variable\n",
    "* It indicates the strength of impact of multiple independent variables on a dependent variable\n",
    "\n",
    "\n",
    "### Pareto/NBD model\n",
    "\n",
    "* The **Pareto/NBD model** is a well-known and frequently applied probabilistic model in the non-contractual context\n",
    "\n",
    "![Pareto](./images/pareto.png)\n",
    "\n",
    "* Customers have different shopping habits, It makes more sense to have a distribution for the transaction rate λ, rather than assuming that a single λ explains everyone’s behaviour\n",
    "* NBD stands for negative binomial distribution, Pareto/NBD only focuses on the purchase count and lifetime\n",
    "* Train the model over a training period with a minimum length that corresponds to three times the typical inter-purchase time of your customers\n",
    "* The training period will give you an estimate for the model parameters. We should then be able to compare what the model predicts vs. what you observed in the training period at the customer level. If the purchase count is in agreement, the next step is to compare predictions with observations made in a validation/holdout period.This period has not been observed by the model. If the model performs well in the validation/holdout period, then you can forecast for a period of time from several months to several years, depending on your business needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "\n",
    "* [An Introduction to Predictive Customer Lifetime Value Modeling](https://www.datascience.com/blog/intro-to-predictive-modeling-for-customer-lifetime-value)\n",
    "* [General intitution behind deciding various distributions](https://stats.stackexchange.com/questions/251506/is-it-possible-to-understand-pareto-nbd-model-conceptually)\n",
    "* [An Intro to Predictive Modeling for Customer Lifetime Value (CLV) -- Tutorial Notebook](https://github.com/datascienceinc/oreilly-intro-to-predictive-clv/blob/master/oreilly-an-intro-to-predictive-clv-tutorial.ipynb)\n",
    "* [CART Introduction](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)\n",
    "* [Predictive Customer Analytics series](https://towardsdatascience.com/predictive-customer-analytics-4064d881b649)\n",
    "* [Regression analysis](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/)\n",
    "\n",
    "Disclaimer: This notebook is created by Vigneshwer on personel interest and not for any commerical purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
